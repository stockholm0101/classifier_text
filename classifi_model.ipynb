{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from models.reader import data_reader\n",
    "import config\n",
    "from utils.data_utils import dump_pkl, write_vocab, load_pkl, build_vocab, load_vocab\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_sep=config.col_sep\n",
    "data_path=config.train_seg_path\n",
    "min_count=config.min_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_content, data_lbl = data_reader(data_path, col_sep)\n",
    "word_lst = []\n",
    "for i in data_content:\n",
    "    word_lst.extend(i.split())\n",
    "\n",
    "# word vocab\n",
    "word_vocab = build_vocab(word_lst, min_count=min_count, sort=True, lower=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(analyzer='word', ngram_range=(1, 2),vocabulary=word_vocab, sublinear_tf=True)\n",
    "data_feature = vectorizer.fit_transform(data_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idf_featu = vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(data_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(idf_featu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle as pkl\n",
    "import codecs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with codecs.open(\"/ssd1/zhubenchang/output/feature_tf_word.pkl\", \"rb\") as f:\n",
    "    vect = pkl.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = vect.fit_transform(data_content[:10000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with codecs.open(\"/ssd1/zhubenchang/output/model_tf_word_logistic_regression.pkl\", \"rb\") as f:\n",
    "    lr_model = pkl.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arr = lr_model.predict(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_lbl = np.array(data_lbl)\n",
    "data_content = np.array(data_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_content_yl = data_content[data_lbl==\"2201\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_content_yl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = vectorizer.fit_transform(data_content)\n",
    "#pre_res = lr_model.predict(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(data_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "whole = data_feature.toarray()\n",
    "top = whole[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(top[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = vect.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(features)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_feature_arr = data_feature.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_lbl = np.array(data_lbl)\n",
    "data_content = np.array(data_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "key_top = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for ele in data_feature_arr[data_lbl==\"2201\"]:\n",
    "    ele_sort = sorted(zip(idf_featu, ele), key = lambda x:x[1], reverse = True)\n",
    "    print(ele_sort[:20])\n",
    "    key_top.append(ele_sort[:20])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data_content_yl[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_content_yl[pre_res!=16]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "key_top[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = lr_model.coef_\n",
    "weight = weights[16]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rank = sorted(zip(vect.get_feature_names(), weight), key = lambda x:x[1], reverse = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rank[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.getsizeof(vect)/(1024**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## simhash"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jieba\n",
    "import jieba.analyse\n",
    "import numpy as np\n",
    "import json\n",
    " \n",
    "class simhash:\n",
    "    def __init__(self,content):\n",
    "        self.simhash=self.simhash(content)\n",
    " \n",
    "    def __str__(self):\n",
    "        return str(self.simhash)\n",
    " \n",
    "    def simhash(self,content):\n",
    "        #seg = jieba.cut(content)\n",
    "        #jieba.analyse.set_stop_words('data/stop_words.txt_utf8')\n",
    "        #keyWord = jieba.analyse.extract_tags(\n",
    "        #    '|'.join(seg), topK=20, withWeight=True, allowPOS=())#在这里对jieba的tfidf.py进行了修改\n",
    "        #将tags = sorted(freq.items(), key=itemgetter(1), reverse=True)修改成tags = sorted(freq.items(), key=itemgetter(1,0), reverse=True)\n",
    "        #即先按照权重排序，再按照词排序\n",
    "        keyWord = content\n",
    "        keyList = []\n",
    "        # print(keyWord)\n",
    "        for feature, weight in keyWord:\n",
    "            #print(feature + str(weight))\n",
    "            weight = int(weight * 20)\n",
    "            feature = self.string_hash(feature)\n",
    "            temp = []\n",
    "            for i in feature:\n",
    "                if(i == '1'):\n",
    "                    temp.append(weight)\n",
    "                else:\n",
    "                    temp.append(-weight)\n",
    "            # print(temp)\n",
    "            keyList.append(temp)\n",
    "        list1 = np.sum(np.array(keyList), axis=0)\n",
    "        #print(list1)\n",
    "        if(keyList==[]): #编码读不出来\n",
    "            return '00'   \n",
    "        simhash = ''\n",
    "        for i in list1:\n",
    "            if(i > 0):\n",
    "                simhash = simhash + '1'\n",
    "            else:\n",
    "                simhash = simhash + '0'\n",
    "        return simhash\n",
    " \n",
    " \n",
    "    def string_hash(self,source):\n",
    "        if source == \"\":\n",
    "            return 0\n",
    "        else:\n",
    "            x = hash(source)\n",
    "            x = bin(x).replace('0b', '').zfill(128)[-128:]\n",
    "            #print(source,x)\n",
    " \n",
    "            return str(x)\n",
    "#     def string_hash(self,source):\n",
    "#         if source == \"\":\n",
    "#             return 0\n",
    "#         else:\n",
    "#             x = ord(source[0]) << 7\n",
    "#             m = 1000003\n",
    "#             mask = 2 ** 128 - 1\n",
    "#             for c in source:\n",
    "#                 x = ((x * m) ^ ord(c)) & mask\n",
    "#             x ^= len(source)\n",
    "#             if x == -1:\n",
    "#                 x = -2\n",
    "#             x = bin(x).replace('0b', '').zfill(64)[-64:]\n",
    "#             #print(source,x)\n",
    " \n",
    "#             return str(x)\n",
    " \n",
    "        '''\n",
    "        以下是使用系统自带hash生成，虽然每次相同的会生成的一样，\n",
    "        不过，对于不同的汉子产生的二进制，在计算海明码的距离会不一样，\n",
    "        即每次产生的海明距离不一致\n",
    "        所以不建议使用。\n",
    "        '''\n",
    "        # x=str(bin(hash(source)).replace('0b','').replace('-','').zfill(64)[-64:])\n",
    "        # print(source,x,len(x))\n",
    "        # return x\n",
    " \n",
    " \n",
    "    def hammingDis(self,com):\n",
    "        t1 = '0b' + self.simhash\n",
    "        t2 = '0b' + com.simhash\n",
    "        n=int(t1, 2) ^ int(t2, 2)\n",
    "        i=0\n",
    "        while n:\n",
    "            n &= (n-1)\n",
    "            i+=1\n",
    "        return i\n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hash(\"什么\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "key_top_1104 = []\n",
    "for ele in data_feature_arr[(data_lbl==\"2201\")|(data_lbl==\"1104\")]:\n",
    "    ele_sort = sorted(zip(idf_featu, ele), key = lambda x:x[1], reverse = True)\n",
    "    #print(ele_sort[:20])\n",
    "    key_top_1104.append(ele_sort[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "key_top_22_1104[6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "com = simhash(key_top_22_1104[6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "count = 0\n",
    "acc_count = 0\n",
    "diff_list = []\n",
    "for text in key_top_22_1104:\n",
    "    count += 1\n",
    "    test_hash = simhash(text)\n",
    "    diff = test_hash.hammingDis(com)\n",
    "    #print(text)\n",
    "    #print(diff)\n",
    "    diff_list.append(diff)\n",
    "    acc_count += 1\n",
    "print(acc_count/float(count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(data_lbl[(data_lbl==\"2201\") | (data_lbl==\"1104\")])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count = 0\n",
    "for ele in zip(diff_list, data_lbl[(data_lbl==\"2201\") | (data_lbl==\"1104\")]):\n",
    "    if ele[1] == \"2201\":\n",
    "        print(ele)\n",
    "        count += 1\n",
    "print(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count = 0\n",
    "for ele in zip(diff_list, data_lbl[(data_lbl==\"2201\") | (data_lbl==\"1104\")]):\n",
    "    if ele[0] <= 25:\n",
    "        print(ele)\n",
    "        count += 1\n",
    "print(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "15/18"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "15/16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ele in zip(diff_list, data_lbl[(data_lbl==\"2201\") | (data_lbl==\"1104\")]):\n",
    "    if ele[1] == \"1104\":\n",
    "        print(ele)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TFIDF + jecard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "key_top_22_1104 = []\n",
    "for ele in data_feature_arr[(data_lbl==\"2201\")|(data_lbl==\"1104\")]:\n",
    "    ele_sort = sorted(zip(idf_featu, ele), key = lambda x:x[1], reverse = True)\n",
    "    #print(ele_sort[:20])\n",
    "    key_top_22_1104.append(ele_sort[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence1 = np.array(key_top_22_1104[6]).T[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_jaccard_similarity(sentence1, sentence2):\n",
    "    word_set1 = set(sentence1)\n",
    "    word_set2 = set(sentence2)\n",
    "\n",
    "    return len(word_set1 & word_set2) / len(word_set1 | word_set2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sim_list = []\n",
    "for sentence in key_top_22_1104:\n",
    "    sim = compute_jaccard_similarity(sentence1, np.array(sentence).T[0])\n",
    "    sim_list.append(sim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ele in zip(sim_list, data_lbl[(data_lbl==\"2201\") | (data_lbl==\"1104\")]):\n",
    "    if ele[1] == \"2201\":\n",
    "        print(ele)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count = 0\n",
    "for ele in zip(sim_list, data_lbl[(data_lbl==\"2201\") | (data_lbl==\"1104\")]):\n",
    "    if ele[0] > 0.08:\n",
    "        print(ele)\n",
    "        count += 1\n",
    "print(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "17/18"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  TFIDF+JECARD 评估"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import LatentDirichletAllocation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda = LatentDirichletAllocation(n_components=3, learning_method='batch', max_iter=25, random_state=0)\n",
    "lda_topics = lda.fit_transform(data_feature_arr[(data_lbl==\"2201\") | (data_lbl==\"1104\")])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda.components_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_top_words(model, feature_names, n_top_words):\n",
    "    #打印每个主题下权重较高的term\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        print(\"Topic #%d:\" % topic_idx)\n",
    "        print(\" \".join([feature_names[i]\n",
    "                        for i in topic.argsort()[:-n_top_words - 1:-1]]))\n",
    "\n",
    "    #打印主题-词语分布矩阵\n",
    "    print(\"#主题-词语分布矩阵: \\n %s\" %model.components_)\n",
    "\n",
    "n_top_words=20\n",
    "print_top_words(lda, idf_featu, n_top_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_label = []\n",
    "for ele in lda_topics:\n",
    "    lda_label.append(np.array(ele).argsort()[::-1][0])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ele in zip(lda_label, data_lbl[(data_lbl==\"2201\") | (data_lbl==\"1104\")]):\n",
    "    print(ele)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  LDA准召 100%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import SpectralClustering\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "X = np.array([[185.4, 72.6],\n",
    "    [155.0, 54.4],\n",
    "    [170.2, 99.9],\n",
    "    [172.2, 97.3],\n",
    "    [157.5, 59.0],\n",
    "    [190.5, 81.6],\n",
    "    [188.0, 77.1],\n",
    "    [167.6, 97.3],\n",
    "    [172.7, 93.3],\n",
    "    [154.9, 59.0]])\n",
    "\n",
    "w, h = 10, 10;\n",
    "\n",
    "#构建相似度矩阵，任意两个样本间的相似度= 100 - 两个样本的欧氏距离\n",
    "Matrix = [[100- math.hypot(X[x][0]- X[y][0], X[x][1]- X[y][1]) for x in range(w)] for y in range(h)]\n",
    "\n",
    "sc = SpectralClustering(3, affinity='precomputed', n_init=10)\n",
    "sc.fit(Matrix)\n",
    "\n",
    "print('spectral clustering')   \n",
    "print(sc.labels_)\n",
    "print(sc.affinity_matrix_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = [1]\n",
    "b = [5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a += b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
